{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Flow in Pipeline\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you will need:\n",
    "- A basic understanding of Machine Learning\n",
    "- An Azure account with an active subscription - [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace - [Configure workspace](../../configuration.ipynb)\n",
    "- A python environment\n",
    "- Installed Azure Machine Learning Python SDK v2\n",
    "- Installed PromptFlow SDK\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial, you should be able to:\n",
    "- Connect to your AML workspace from the Python SDK\n",
    "- Load a flow as a `ParallelComponent`\n",
    "- Using the component along with other components loaded from yaml in one `PipelineJob`.\n",
    "\n",
    "**Motivations** - This guide will introduce how to use a flow along with other data processing steps in a pipeline.\n",
    "\n",
    "**Known issues** - This feature is not stable now and here are known issues we are actively fixing:\n",
    "- You must include a `.promptflow/flow.tools.json` in the flow directory first. This file will automatically generated when you run the flow locally.\n",
    "- Component of the same name (even with different version) can be created only once. An auto-generated component name based on hash will be used when component name & version are neither provided.\n",
    "- The flow nodes can only run on computer cluster with managed identity assigned Azure ML Data Scientist role.\n",
    "- connection/columns_mapping overwrite doesn't work for now.\n",
    "- This feature works on canary workspace only for now: [sample job link](https://ml.azure.com/experiments/id/9ce1a534-9d3d-4761-a5e7-5299dd6912f1/runs/clever_leek_4xh6x9z7s5?wsid=/subscriptions/96aede12-2f73-41cb-b983-6d11a904839b/resourcegroups/promptflow/workspaces/promptflow-canary-dev&tid=72f988bf-86f1-41af-91ab-2d7cd011db47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install dependent packages\n",
    "\n",
    "Please follow [configuration.ipynb](../../configuration.ipynb) to install dependent packages and connect to a workspace first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"promptflow[azure]\" promptflow-tools--extra-index-url https://azuremlsdktestpypi.azureedge.net/promptflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to MLClient and create necessary connections\n",
    "Similar to other SDK in azure-ai-ml, you need to import related packages and prepare a ML client connecting to a specific workspace first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "from promptflow.azure import PFClient\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "# Get a handle to workspace\n",
    "ml_client = MLClient.from_config(credential=credential)\n",
    "\n",
    "# Create PFClient connected to workspace\n",
    "pf = PFClient(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load flow as a component\n",
    "\n",
    "Suppose you have already authored a flow, you can load it as component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flow_component = pf.load_as_component(\n",
    "    \"../../flows/standard/web-classification/\",\n",
    "    columns_mapping={\n",
    "        \"url\": \"${data.url}\",\n",
    "        \"groundtruth\": \"${data.answer}\",\n",
    "    },\n",
    "    component_type=\"parallel\",\n",
    ")\n",
    "\n",
    "print(flow_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use the component in a pipeline\n",
    "\n",
    "Then you can use this component along with other components in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv2jsonl_component = load_component(\"./tsv2jsonl-component/component_spec.yaml\")\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def pipeline_with_flow(input_data):\n",
    "    data_transfer = tsv2jsonl_component(input_data=input_data)\n",
    "\n",
    "    flow_node = flow_component(\n",
    "        # this can be either a URI jsonl file or a URI folder containing multiple jsonl files\n",
    "        data=data_transfer.outputs.output_data,\n",
    "        # you can overwrite inputs mapping here\n",
    "        groundtruth=\"Channel\",\n",
    "        # this is to overwrite connection settings\n",
    "        connections={\n",
    "            # this is to overwrite connection related settings for a LLM node\n",
    "            # \"summarize_text_content\" is the node name\n",
    "            \"summarize_text_content\": {\n",
    "                \"deployment_name\": \"another_deployment_name\",\n",
    "            },\n",
    "            # you can overwrite custom connection input of a python node here\n",
    "            # \"convert_to_dict\": {\n",
    "            #     \"conn1\": \"another_connection\"\n",
    "            # }\n",
    "        },\n",
    "    )\n",
    "    # node level run settings for flow node is similar to `ParallelComponent`\n",
    "    flow_node.logging_level = \"DEBUG\"\n",
    "    flow_node.max_concurrency_per_instance = 2\n",
    "    return flow_node.outputs\n",
    "\n",
    "\n",
    "pipeline = pipeline_with_flow(\n",
    "    input_data=Input(path=\"./data.tsv\", type=AssetTypes.URI_FILE),\n",
    ")\n",
    "\n",
    "pipeline.settings.default_compute = \"cpu-cluster\"\n",
    "\n",
    "created_job = ml_client.jobs.create_or_update(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like other pipeline jobs in azure-ai-ml, you can monitor the status of the job via `ml_client.jobs.stream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(created_job.name)\n",
    "ml_client.jobs.wait_for_completion(created_job.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
