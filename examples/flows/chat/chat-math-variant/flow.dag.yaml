environment:
  python_requirements_txt: requirements.txt
inputs:
  chat_history:
    type: list
    is_chat_history: true
    default: []
  question:
    type: string
    is_chat_input: true
outputs:
  answer:
    type: string
    reference: ${extract_result.output}
    is_chat_output: true
nodes:
- name: chat
  use_variants: true
- name: extract_result
  type: python
  source:
    type: code
    path: extract_result.py
  inputs:
    input1: ${chat.output}
node_variants:
  chat:
    default_variant_id: variant_0
    variants:
      variant_0:
        node:
          type: llm
          source:
            type: code
            path: chat.jinja2
          inputs:
            deployment_name: gpt-35-turbo
            max_tokens: 256
            temperature: 0
            chat_history: ${inputs.chat_history}
            question: ${inputs.question}
            model: gpt-4
          connection: open_ai_connection
          api: chat
      variant_1:
        node:
          type: llm
          source:
            type: code
            path: chat_variant_1.jinja2
          inputs:
            deployment_name: gpt-35-turbo
            max_tokens: 256
            temperature: 0
            chat_history: ${inputs.chat_history}
            question: ${inputs.question}
            model: gpt-4
          connection: open_ai_connection
          api: chat
      variant_2:
        node:
          type: llm
          source:
            type: code
            path: chat_variant_2.jinja2
          inputs:
            deployment_name: gpt-35-turbo
            max_tokens: 256
            temperature: 0
            chat_history: ${inputs.chat_history}
            question: ${inputs.question}
            model: gpt-4
          connection: open_ai_connection
          api: chat
