inputs:
  selected_prompt_id:
    type: string
    default: S2.1__U1.0
  dynamic_template_variables:
    type: object
    default: {}
  llm_model_id:
    type: string
    default: gemini-2.5-pro-preview-05-06
  temperature:
    type: double
    default: 0.7
  max_tokens:
    type: int
    default: 4000
  run_id:
    type: string
    default: multi_llm_run1
  flow_name:
    type: string
    default: multi_llm_flow
  is_rag_flow:
    type: bool
    default: false
  batch_id:
    type: string
    default: random_test_1
outputs:
  final_response:
    type: string
    reference: ${multi_llm_node.output.response_text}
  invocation_status:
    type: bool
    reference: ${multi_llm_node.output.status}}
  report_status:
    type: string
    value: ${report_generator_node.output.status}
    reference: ${multi_llm_report_node.output.status}
  report_path:
    type: string
    value: ${report_generator_node.output.report_path}
    reference: ${multi_llm_report_node.output.report_path}
  report_id:
    type: int
    value: ${report_generator_node.output.report_id}
    reference: ${multi_llm_report_node.output.report_id}
nodes:
- name: prompt_selector_node
  type: python
  source:
    type: code
    path: ../../tools/prompt_selector_tool.py
  inputs: {}
- name: multi_llm_node
  type: python
  source:
    type: code
    path: nodes/multi_llm_invocation.py
  inputs:
    user_prompt_template_path: ${prompt_selector_node.output.user_prompt_template_path}
    system_prompt_template_path: ${prompt_selector_node.output.system_prompt_template_path}
    prompt_id: ${inputs.selected_prompt_id}
    model_id: ${inputs.llm_model_id}
    selector_template_variables: ${prompt_selector_node.output.template_variables}
    dynamic_template_variables: ${inputs.dynamic_template_variables}
- name: multi_llm_report_node
  type: python
  source:
    type: code
    path: ../../scripts/generate_report.py
  inputs:
    llm_model_id: ${multi_llm_node.output.model_id_used}
    system_prompt_used: ${multi_llm_node.output.system_prompt_used}
    user_prompt_used: ${multi_llm_node.output.user_prompt_used}
    llm_invocation_output: ${multi_llm_node.output.full_api_response}
    llm_run_time: ${multi_llm_node.output.llm_run_time}
    flow_name: ${inputs.flow_name}
    is_rag_flow: ${inputs.is_rag_flow}
    batch_id: ${inputs.batch_id}
